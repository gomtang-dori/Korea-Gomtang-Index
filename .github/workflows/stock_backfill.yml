name: Stock Backfill (Recent5Y chunked + stable + cache raw/curated)

on:
  workflow_dispatch:
    inputs:
      skip_prices:
        description: "Skip price data fetch"
        type: boolean
        default: false
      skip_flows:
        description: "Skip flows data fetch"
        type: boolean
        default: false
      skip_fundamentals:
        description: "Skip fundamentals data fetch"
        type: boolean
        default: false
      skip_dart:
        description: "Skip DART data fetch (recommended: true; DART separate backfill)"
        type: boolean
        default: true
      clean_derived:
        description: "Delete derived outputs (curated/analysis/docs) before curations"
        type: boolean
        default: true
      # 캐시 만들기 목적이면 리포트/CSV export는 끄는게 안전(시간 초과 방지)
      skip_heavy_outputs:
        description: "Skip heavy steps (render reports / big CSV exports) to avoid timeout"
        type: boolean
        default: true
      # 수동으로 범위를 고정하고 싶을 때만 사용(기본은 자동 recent5y)
      range_mode:
        description: "recent5y(auto) or custom"
        type: choice
        options: [recent5y, custom]
        default: recent5y
      year_from:
        description: "custom mode only (YYYY)"
        required: false
        default: ""
      year_to:
        description: "custom mode only (YYYY)"
        required: false
        default: ""

jobs:
  backfill:
    runs-on: ubuntu-latest
    timeout-minutes: 330

    env:
      PYTHONPATH: src
      SAMPLE_MODE: "false"

      # cache namespace (원하면 올려서 캐시 리셋)
      STOCKS_BACKFILL_CACHE_NS: "v1"

      # ---------- 안정 세팅(핵심) ----------
      # prices
      MAX_WORKERS: "20"
      PRICE_APPEND_IF_EXISTS: "true"
      PRICE_RETRY: "3"
      PRICE_NO_DATA_RETRY: "1"
      PRICE_NO_DATA_SLEEP_BASE: "3.0"
      PRICE_THROTTLE_SEC: "0.06"   # 가격은 호출량 많아서 너무 높이면 느림. 막히면 0.10~0.20으로

      # flows
      MAX_WORKERS_FLOWS: "3"
      KRX_FLOWS_SAVE_FORMAT: "parquet"
      KRX_RETRY: "3"
      KRX_NO_DATA_RETRY: "1"
      KRX_NO_DATA_SLEEP_BASE: "4.0"
      KRX_THROTTLE_SEC: "0.20"     # 막히면 0.25~0.35
      FLOW_APPEND_IF_EXISTS: "true"

      # flows retry2 (no-data tickers only) - 유지
      MAX_WORKERS_FLOWS_RETRY: "3"
      KRX_THROTTLE_SEC_RETRY2: "0.25"
      KRX_NO_DATA_RETRY_RETRY2: "3"
      KRX_NO_DATA_SLEEP_BASE_RETRY2: "5.0"
      RETRY2_OVERWRITE_IF_EXISTS: "false"

      # fundamentals
      MAX_WORKERS_FUNDAMENTALS: "8"
      FUND_CHUNK_YEARS: "1"        # 연도 분할 실행이므로 내부 chunk는 1년이 가장 단순
      FUND_SAVE_FORMAT: "parquet"
      FUND_RETRY: "3"
      FUND_EMPTY_RETRY: "2"
      FUND_EMPTY_SLEEP_BASE: "2.5"
      FUND_NO_DATA_RETRY: "1"
      FUND_NO_DATA_SLEEP_BASE: "3.0"
      FUND_THROTTLE_SEC: "0.08"
      FUND_APPEND_IF_EXISTS: "true"

      # progress/heartbeat (모니터링)
      HEARTBEAT_SEC: "60"

      # DART (별도 backfill이면 여기선 스킵 권장)
      DART_API_KEY: ${{ secrets.DART_API_KEY }}

    steps:
      - uses: actions/checkout@v4

      - uses: actions/setup-python@v5
        with:
          python-version: "3.10"

      - name: Install dependencies
        run: |
          pip install pandas pyarrow pykrx requests plotly tabulate tqdm

      - name: Restore cache (stocks raw + curated)
        id: stocks_cache_restore
        uses: actions/cache/restore@v4
        with:
          path: |
            data/stocks/raw/prices
            data/stocks/raw/krx_flows
            data/stocks/raw/fundamentals
            data/stocks/raw/dart
            data/stocks/curated
            docs/stocks/dart_standard_*.csv
          key: stocks-backfill-${{ env.STOCKS_BACKFILL_CACHE_NS }}-${{ github.ref_name }}-${{ github.run_id }}
          restore-keys: |
            stocks-backfill-${{ env.STOCKS_BACKFILL_CACHE_NS }}-${{ github.ref_name }}-
            stocks-backfill-${{ env.STOCKS_BACKFILL_CACHE_NS }}-

      - name: Cache precheck summary
        if: always()
        run: |
          echo "[cache] hit? -> ${{ steps.stocks_cache_restore.outputs.cache-hit }}"
          python - <<'PY'
          from pathlib import Path

          def count_files(p: Path, pattern: str):
            if not p.exists(): return 0
            return len(list(p.glob(pattern)))

          def count_rglob(p: Path, suffix: str):
            if not p.exists(): return 0
            return sum(1 for x in p.rglob(f"*{suffix}") if x.is_file())

          print("=== precheck raw/curated ===")
          print("[raw] prices csv files      =", count_files(Path("data/stocks/raw/prices"), "*.csv"))
          print("[raw] krx_flows parquet     =", count_files(Path("data/stocks/raw/krx_flows"), "*.parquet"))
          print("[raw] fundamentals parquet  =", count_files(Path("data/stocks/raw/fundamentals"), "*.parquet"))
          print("[raw] dart json count       =", count_rglob(Path("data/stocks/raw/dart"), ".json"))
          print("[curated] flows_daily       =", count_rglob(Path("data/stocks/curated"), "flows_daily.parquet"))
          print("[curated] fundamentals_daily=", count_rglob(Path("data/stocks/curated"), "fundamentals_daily.parquet"))
          print("[docs] dart_standard csv    =", count_files(Path("docs/stocks"), "dart_standard_*.csv"))
          print("================================")
          PY
          du -sh data/stocks/raw 2>/dev/null || true
          du -sh data/stocks/curated 2>/dev/null || true

      - name: Resolve year range (recent5y auto OR custom)
        run: |
          set -euxo pipefail
          MODE="${{ inputs.range_mode }}"
          if [ "$MODE" = "custom" ] && [ -n "${{ inputs.year_from }}" ] && [ -n "${{ inputs.year_to }}" ]; then
            YEAR_FROM="${{ inputs.year_from }}"
            YEAR_TO="${{ inputs.year_to }}"
          else
            YEAR_TO="$(date -u +%Y)"
            YEAR_FROM="$((YEAR_TO-4))"
          fi
          echo "YEAR_FROM=$YEAR_FROM" >> $GITHUB_ENV
          echo "YEAR_TO=$YEAR_TO" >> $GITHUB_ENV
          echo "Resolved range: $YEAR_FROM ~ $YEAR_TO (mode=$MODE)"

      - name: Init RUN TAG (for marker)
        run: |
          echo "RUN_TAG=${GITHUB_RUN_ID}_${GITHUB_RUN_ATTEMPT}" >> $GITHUB_ENV
          echo "RUN_TAG=$RUN_TAG"

      - name: "[cleanup] Backup existing derived outputs (curated/analysis/docs)"
        if: ${{ inputs.clean_derived }}
        continue-on-error: true
        run: |
          echo "=== Pre-clean backup (derived outputs) ==="
          tar -czf pre_clean_derived_backup.tar.gz \
            data/stocks/curated \
            data/stocks/analysis \
            docs/stocks \
            2>/dev/null || true
          ls -lh pre_clean_derived_backup.tar.gz 2>/dev/null || true

      - name: "[cleanup] Upload pre-clean backup artifact"
        if: ${{ always() && inputs.clean_derived }}
        uses: actions/upload-artifact@v4
        with:
          name: pre-clean-derived-backup
          path: pre_clean_derived_backup.tar.gz
          retention-days: 7
          if-no-files-found: warn

      - name: "[cleanup] Remove derived outputs to rebuild cleanly"
        if: ${{ inputs.clean_derived }}
        run: |
          rm -rf data/stocks/curated || true
          rm -rf data/stocks/analysis || true
          rm -rf docs/stocks || true
          mkdir -p data/stocks/curated data/stocks/analysis docs/stocks
          echo "OK: derived outputs cleaned & dirs recreated"

      - name: "[marker] Write run marker"
        if: always()
        run: |
          mkdir -p docs/stocks
          cat > docs/stocks/_run_marker.txt <<EOF
          run_tag=${RUN_TAG}
          run_id=${GITHUB_RUN_ID}
          run_attempt=${GITHUB_RUN_ATTEMPT}
          utc=$(date -u +"%Y-%m-%dT%H:%M:%SZ")
          range_year_from=${YEAR_FROM}
          range_year_to=${YEAR_TO}
          EOF
          cat docs/stocks/_run_marker.txt

      - name: "[1/12] Fetch listings"
        continue-on-error: true
        run: |
          python -u src/stocks/fetch_listings.py || true

      - name: "[2/12] Fetch prices (recent5y, year-chunked, stable)"
        if: ${{ !inputs.skip_prices }}
        continue-on-error: true
        env:
          INCREMENTAL_MODE: "false"
          PROGRESS_JSON: "docs/stocks/progress_prices.json"
        run: |
          set -euxo pipefail
          for Y in $(seq $YEAR_FROM $YEAR_TO); do
            echo "::group::prices $Y"
            export START_DATE="${Y}0101"
            export END_DATE="${Y}1231"
            python -u src/stocks/fetch_prices.py || true
            echo "::endgroup::"
          done
          cat docs/stocks/progress_prices.json || true

      - name: "[3/12] Fetch KRX flows (recent5y, year-chunked + retry2, stable)"
        if: ${{ !inputs.skip_flows }}
        continue-on-error: true
        env:
          INCREMENTAL_MODE: "false"
          PROGRESS_JSON: "docs/stocks/progress_flows.json"
        run: |
          set -euxo pipefail
          for Y in $(seq $YEAR_FROM $YEAR_TO); do
            echo "::group::flows $Y (main)"
            export START_DATE="${Y}0101"
            export END_DATE="${Y}1231"
            python -u src/stocks/fetch_krx_flows.py || true
            echo "::endgroup::"

            echo "::group::flows $Y (retry2 no-data only)"
            # retry2도 같은 기간으로 (script가 END_DATE 존중하도록 패치되어 있어야 함)
            export START_DATE="${Y}0101"
            export END_DATE="${Y}1231"
            python -u src/stocks/fetch_krx_flows_retry_nodata.py || true
            echo "::endgroup::"
          done
          cat docs/stocks/progress_flows.json || true

      - name: "[4/12] Fetch fundamentals (recent5y, year-chunked, stable)"
        if: ${{ !inputs.skip_fundamentals }}
        continue-on-error: true
        env:
          INCREMENTAL_MODE: "false"
          PROGRESS_JSON: "docs/stocks/progress_fundamentals.json"
        run: |
          set -euxo pipefail
          for Y in $(seq $YEAR_FROM $YEAR_TO); do
            echo "::group::fundamentals $Y"
            export START_DATE="${Y}0101"
            export END_DATE="${Y}1231"
            python -u src/stocks/fetch_fundamentals.py || true
            echo "::endgroup::"
          done
          cat docs/stocks/progress_fundamentals.json || true

      - name: "[5/12] Fetch DART financials (optional)"
        if: ${{ !inputs.skip_dart }}
        continue-on-error: true
        run: |
          python -u src/stocks/fetch_dart_financials.py || true

      - name: "[mid-save] Raw data backup"
        continue-on-error: true
        run: |
          du -sh data/stocks/ || true
          tar -czf raw_data_backup.tar.gz data/stocks/raw/ || true

      - name: "[mid-save] Upload raw-data-backup"
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: raw-data-backup
          path: raw_data_backup.tar.gz
          retention-days: 7
          if-no-files-found: warn

      - name: "Upload progress snapshots"
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: backfill-progress
          path: |
            docs/stocks/progress_prices.json
            docs/stocks/progress_flows.json
            docs/stocks/progress_fundamentals.json
            docs/stocks/_run_marker.txt
          retention-days: 30
          if-no-files-found: warn

      # ---- Curations / features ----
      - name: "[6/12] Curate flows"
        continue-on-error: true
        run: |
          python -u src/stocks/curate_flows.py || true

      - name: "[7/12] Curate fundamentals"
        continue-on-error: true
        run: |
          python -u src/stocks/curate_fundamentals.py || true

      - name: "[8/12] Curate financials (DART-derived) (optional)"
        if: ${{ !inputs.skip_dart }}
        continue-on-error: true
        run: |
          python -u src/stocks/curate_financials.py || true

      - name: "[9/12] Compute features"
        continue-on-error: true
        run: |
          python -u src/stocks/compute_features.py || true

      # ---- Heavy outputs (optional) ----
      - name: "[10/12] Render reports (heavy)"
        if: ${{ !inputs.skip_heavy_outputs }}
        continue-on-error: true
        run: |
          python -u src/stocks/render_stock_report.py || true

      - name: "[11/12] Export CSV summaries (heavy)"
        if: ${{ !inputs.skip_heavy_outputs }}
        continue-on-error: true
        run: |
          python -u src/stocks/export_data_summary.py || true

      - name: "[12/12] Export raw data to CSV (very heavy)"
        if: ${{ !inputs.skip_heavy_outputs }}
        continue-on-error: true
        run: |
          python -u src/stocks/export_raw_to_csv.py || true

      - name: Save cache (stocks raw + curated)
        if: always()
        uses: actions/cache/save@v4
        with:
          path: |
            data/stocks/raw/prices
            data/stocks/raw/krx_flows
            data/stocks/raw/fundamentals
            data/stocks/raw/dart
            data/stocks/curated
            docs/stocks/dart_standard_*.csv
          key: stocks-backfill-${{ env.STOCKS_BACKFILL_CACHE_NS }}-${{ github.ref_name }}-${{ github.run_id }}

      - name: Summary (always)
        if: always()
        run: |
          echo "=== DONE summary ==="
          echo "range: $YEAR_FROM ~ $YEAR_TO"
          echo "prices files: $(ls data/stocks/raw/prices/ 2>/dev/null | wc -l)"
          echo "flows files(parquet): $(ls data/stocks/raw/krx_flows/*.parquet 2>/dev/null | wc -l)"
          echo "fundamentals files(parquet): $(ls data/stocks/raw/fundamentals/*.parquet 2>/dev/null | wc -l)"
          echo "curated flows: $(find data/stocks/curated -name flows_daily.parquet 2>/dev/null | wc -l)"
          echo "curated fundamentals: $(find data/stocks/curated -name fundamentals_daily.parquet 2>/dev/null | wc -l)"
          du -sh data/stocks/raw 2>/dev/null || true
          du -sh data/stocks/curated 2>/dev/null || true
