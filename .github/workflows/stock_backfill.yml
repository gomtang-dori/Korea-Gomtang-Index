name: Stock Backfill RAW (Matrix 2022-2026) + Merge + Cache (max-parallel=2)

on:
  workflow_dispatch:
    inputs:
      skip_prices:
        description: "Skip price data fetch"
        type: boolean
        default: false
      skip_flows:
        description: "Skip KRX flows fetch"
        type: boolean
        default: false
      skip_fundamentals:
        description: "Skip fundamentals fetch"
        type: boolean
        default: false

      skip_dart:
        description: "Skip DART fetch (recommended true; DART separate backfill)"
        type: boolean
        default: true

      clean_derived:
        description: "Delete derived outputs (curated/analysis/docs) before curations (recommended false for RAW backfill)"
        type: boolean
        default: false

permissions:
  contents: read

concurrency:
  group: stock-backfill-raw-matrix-${{ github.ref_name }}
  cancel-in-progress: false

jobs:
  # ------------------------------------------------------------
  # (A) 연도별 RAW 생성 job (서로 다른 폴더에만 씀 -> 파일 충돌 방지)
  # ------------------------------------------------------------
  backfill_year:
    runs-on: ubuntu-latest
    timeout-minutes: 330

    strategy:
      fail-fast: false
      max-parallel: 2
      matrix:
        year: [2022, 2023, 2024, 2025, 2026]

    env:
      PYTHONPATH: src
      SAMPLE_MODE: "false"

      STOCKS_BACKFILL_CACHE_NS: "v1"

      # ---------- 안정 세팅 ----------
      # prices
      MAX_WORKERS: "20"
      PRICE_APPEND_IF_EXISTS: "true"
      PRICE_RETRY: "3"
      PRICE_NO_DATA_RETRY: "1"
      PRICE_NO_DATA_SLEEP_BASE: "3.0"
      PRICE_THROTTLE_SEC: "0.06"

      # flows
      MAX_WORKERS_FLOWS: "6"
      KRX_FLOWS_SAVE_FORMAT: "parquet"
      KRX_RETRY: "3"
      KRX_NO_DATA_RETRY: "1"
      KRX_NO_DATA_SLEEP_BASE: "4.0"
      KRX_THROTTLE_SEC: "0.13"
      FLOW_APPEND_IF_EXISTS: "true"

      # flows retry2
      MAX_WORKERS_FLOWS_RETRY: "3"
      KRX_THROTTLE_SEC_RETRY2: "0.25"
      KRX_NO_DATA_RETRY_RETRY2: "3"
      KRX_NO_DATA_SLEEP_BASE_RETRY2: "5.0"
      RETRY2_OVERWRITE_IF_EXISTS: "false"

      # fundamentals
      MAX_WORKERS_FUNDAMENTALS: "8"
      FUND_CHUNK_YEARS: "1"
      FUND_SAVE_FORMAT: "parquet"
      FUND_RETRY: "3"
      FUND_EMPTY_RETRY: "2"
      FUND_EMPTY_SLEEP_BASE: "2.5"
      FUND_NO_DATA_RETRY: "1"
      FUND_NO_DATA_SLEEP_BASE: "3.0"
      FUND_THROTTLE_SEC: "0.08"
      FUND_APPEND_IF_EXISTS: "true"

      HEARTBEAT_SEC: "60"
      DART_API_KEY: ${{ secrets.DART_API_KEY }}

    steps:
      - uses: actions/checkout@v4

      - uses: actions/setup-python@v5
        with:
          python-version: "3.10"

      - name: Install dependencies
        run: |
          pip install pandas pyarrow pykrx requests tabulate tqdm

      # seed: 이전 run 캐시가 있으면 그걸 바탕으로 SKIP/append가 더 잘 작동
      - name: Restore cache (seed)
        id: cache_restore
        uses: actions/cache/restore@v4
        with:
          path: |
            data/stocks/master
            data/stocks/raw
            data/stocks/curated
            docs/stocks/dart_standard_*.csv
          key: stocks-backfill-${{ env.STOCKS_BACKFILL_CACHE_NS }}-${{ github.ref_name }}-${{ github.run_id }}
          restore-keys: |
            stocks-backfill-${{ env.STOCKS_BACKFILL_CACHE_NS }}-${{ github.ref_name }}-
            stocks-backfill-${{ env.STOCKS_BACKFILL_CACHE_NS }}-

      - name: Cache precheck summary
        if: always()
        run: |
          echo "[cache] hit? -> ${{ steps.cache_restore.outputs.cache-hit }}"
          python - <<'PY'
          from pathlib import Path
          def count_files(p: Path, pattern: str):
            if not p.exists(): return 0
            return len(list(p.glob(pattern)))
          def count_rglob(p: Path, suffix: str):
            if not p.exists(): return 0
            return sum(1 for x in p.rglob(f"*{suffix}") if x.is_file())
          print("=== precheck ===")
          print("[raw] prices csv files      =", count_files(Path("data/stocks/raw/prices"), "*.csv"))
          print("[raw] krx_flows parquet     =", count_files(Path("data/stocks/raw/krx_flows"), "*.parquet"))
          print("[raw] fundamentals parquet  =", count_files(Path("data/stocks/raw/fundamentals"), "*.parquet"))
          print("[curated] fundamentals_daily=", count_rglob(Path("data/stocks/curated"), "fundamentals_daily.parquet"))
          print("================")
          PY

      - name: "[cleanup] Remove derived outputs (optional)"
        if: ${{ inputs.clean_derived }}
        run: |
          rm -rf data/stocks/curated || true
          rm -rf data/stocks/analysis || true
          rm -rf docs/stocks || true
          mkdir -p data/stocks/curated data/stocks/analysis docs/stocks
          echo "OK: derived outputs cleaned"

      - name: Fetch listings
        continue-on-error: true
        run: |
          python -u src/stocks/fetch_listings.py || true

      - name: Resolve year window for this job
        run: |
          set -euxo pipefail
          Y="${{ matrix.year }}"
          echo "YEAR=$Y" >> $GITHUB_ENV
          echo "START_DATE=${Y}0101" >> $GITHUB_ENV
          echo "END_DATE=${Y}1231" >> $GITHUB_ENV
          echo "This job year=$Y range=${Y}0101~${Y}1231"

      - name: Prepare year-partitioned raw folders
        run: |
          set -euxo pipefail
          mkdir -p data/stocks/raw_year/prices/${YEAR}
          mkdir -p data/stocks/raw_year/krx_flows/${YEAR}
          mkdir -p data/stocks/raw_year/fundamentals/${YEAR}
          mkdir -p docs/stocks

      - name: "Fetch prices (year=${{ matrix.year }})"
        if: ${{ !inputs.skip_prices }}
        continue-on-error: true
        env:
          PROGRESS_JSON: docs/stocks/progress_prices_${{ matrix.year }}.json
          PRICES_RAW_DIR: data/stocks/raw_year/prices/${{ matrix.year }}
        run: |
          set -euxo pipefail
          python -u src/stocks/fetch_prices.py || true
          cat "${PROGRESS_JSON}" || true

      - name: "Fetch KRX flows (year=${{ matrix.year }})"
        if: ${{ !inputs.skip_flows }}
        continue-on-error: true
        env:
          PROGRESS_JSON: docs/stocks/progress_flows_${{ matrix.year }}.json
          FLOWS_RAW_DIR: data/stocks/raw_year/krx_flows/${{ matrix.year }}
          RETRY2_INPUT_FILE: data/stocks/raw_year/krx_flows/${{ matrix.year }}/_no_data_tickers.txt
        run: |
          set -euxo pipefail
          python -u src/stocks/fetch_krx_flows.py || true
          python -u src/stocks/fetch_krx_flows_retry_nodata.py || true
          cat "${PROGRESS_JSON}" || true

      - name: "Fetch fundamentals (year=${{ matrix.year }})"
        if: ${{ !inputs.skip_fundamentals }}
        continue-on-error: true
        env:
          PROGRESS_JSON: docs/stocks/progress_fund_${{ matrix.year }}.json
          FUND_RAW_DIR: data/stocks/raw_year/fundamentals/${{ matrix.year }}
        run: |
          set -euxo pipefail
          python -u src/stocks/fetch_fundamentals.py || true
          cat "${PROGRESS_JSON}" || true

      - name: Upload year raw artifacts
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: raw-year-${{ matrix.year }}
          path: |
            data/stocks/raw_year/prices/${{ matrix.year }}
            data/stocks/raw_year/krx_flows/${{ matrix.year }}
            data/stocks/raw_year/fundamentals/${{ matrix.year }}
            docs/stocks/progress_prices_${{ matrix.year }}.json
            docs/stocks/progress_flows_${{ matrix.year }}.json
            docs/stocks/progress_fund_${{ matrix.year }}.json
          retention-days: 30
          if-no-files-found: warn

  # ------------------------------------------------------------
  # (B) merge job 1개: 연도별 결과를 canonical(raw/)로 합치고 cache 저장
  # ------------------------------------------------------------
  merge_and_cache:
    runs-on: ubuntu-latest
    timeout-minutes: 330
    needs: [backfill_year]

    env:
      PYTHONPATH: src
      STOCKS_BACKFILL_CACHE_NS: "v1"

    steps:
      - uses: actions/checkout@v4

      - uses: actions/setup-python@v5
        with:
          python-version: "3.10"

      - name: Install dependencies
        run: |
          pip install pandas pyarrow tqdm

      - name: Restore cache (seed canonical raw)
        id: cache_restore
        uses: actions/cache/restore@v4
        with:
          path: |
            data/stocks/master
            data/stocks/raw
            data/stocks/curated
            docs/stocks/dart_standard_*.csv
          key: stocks-backfill-${{ env.STOCKS_BACKFILL_CACHE_NS }}-${{ github.ref_name }}-${{ github.run_id }}
          restore-keys: |
            stocks-backfill-${{ env.STOCKS_BACKFILL_CACHE_NS }}-${{ github.ref_name }}-
            stocks-backfill-${{ env.STOCKS_BACKFILL_CACHE_NS }}-

      - name: Download all year artifacts
        uses: actions/download-artifact@v4
        with:
          path: /tmp/raw_year_artifacts

      - name: List downloaded artifacts
        run: |
          set -euxo pipefail
          ls -lah /tmp/raw_year_artifacts || true
          find /tmp/raw_year_artifacts -maxdepth 4 -type d | head -80

      - name: Merge year-partitioned raw -> canonical raw
        run: |
          set -euxo pipefail
          mkdir -p data/stocks/raw/prices data/stocks/raw/krx_flows data/stocks/raw/fundamentals
          python - <<'PY'
          from pathlib import Path
          import pandas as pd

          ART_ROOT = Path("/tmp/raw_year_artifacts")
          PRICES_CAN = Path("data/stocks/raw/prices")
          FLOWS_CAN  = Path("data/stocks/raw/krx_flows")
          FUND_CAN   = Path("data/stocks/raw/fundamentals")

          YEARS = [2022, 2023, 2024, 2025, 2026]

          def iter_year_dirs(kind: str):
            for y in YEARS:
              pattern = f"raw-year-{y}/**/data/stocks/raw_year/{kind}/{y}"
              for h in ART_ROOT.glob(pattern):
                if h.is_dir():
                  yield y, h

          def read_csv_safe(p: Path):
            try:
              df = pd.read_csv(p, encoding="utf-8-sig")
              if "date" in df.columns:
                df["date"] = pd.to_datetime(df["date"])
              return df
            except Exception:
              return None

          def read_parquet_safe(p: Path):
            try:
              df = pd.read_parquet(p)
              if "date" in df.columns:
                df["date"] = pd.to_datetime(df["date"])
              return df
            except Exception:
              return None

          def merge_prices():
            by_ticker = {}
            for _, d in iter_year_dirs("prices"):
              for p in d.glob("*.csv"):
                by_ticker.setdefault(p.stem, []).append(p)
            print("[merge] prices tickers:", len(by_ticker))

            for i, (t, files) in enumerate(by_ticker.items(), 1):
              frames = []
              can_path = PRICES_CAN / f"{t}.csv"
              if can_path.exists():
                old = read_csv_safe(can_path)
                if old is not None and not old.empty:
                  frames.append(old)

              for f in sorted(files):
                df = read_csv_safe(f)
                if df is not None and not df.empty:
                  frames.append(df)

              if not frames:
                continue

              out = pd.concat(frames, ignore_index=True)
              if "date" in out.columns:
                out = out.drop_duplicates(subset=["date"], keep="last").sort_values("date")

              can_path.parent.mkdir(parents=True, exist_ok=True)
              out.to_csv(can_path, index=False, encoding="utf-8-sig")

              if i <= 10 or i % 400 == 0:
                print(f"[merge][prices] {i}/{len(by_ticker)} {t} rows={len(out)}")

          def merge_parquet(kind: str, can_dir: Path):
            by_ticker = {}
            for _, d in iter_year_dirs(kind):
              for p in d.glob("*.parquet"):
                by_ticker.setdefault(p.stem, []).append(p)
            print(f"[merge] {kind} tickers:", len(by_ticker))

            for i, (t, files) in enumerate(by_ticker.items(), 1):
              frames = []
              can_path = can_dir / f"{t}.parquet"
              if can_path.exists():
                old = read_parquet_safe(can_path)
                if old is not None and not old.empty:
                  frames.append(old)

              for f in sorted(files):
                df = read_parquet_safe(f)
                if df is not None and not df.empty:
                  frames.append(df)

              if not frames:
                continue

              out = pd.concat(frames, ignore_index=True)
              if "date" in out.columns:
                out = out.drop_duplicates(subset=["date"], keep="last").sort_values("date")

              can_path.parent.mkdir(parents=True, exist_ok=True)
              out.to_parquet(can_path, index=False)

              if i <= 10 or i % 400 == 0:
                print(f"[merge][{kind}] {i}/{len(by_ticker)} {t} rows={len(out)}")

          merge_prices()
          merge_parquet("krx_flows", FLOWS_CAN)
          merge_parquet("fundamentals", FUND_CAN)

          print("[merge] done")
          print("prices files:", len(list(PRICES_CAN.glob("*.csv"))))
          print("flows files :", len(list(FLOWS_CAN.glob("*.parquet"))))
          print("fund files  :", len(list(FUND_CAN.glob("*.parquet"))))
          PY

      - name: Post-merge sanity counts
        run: |
          set -euxo pipefail
          echo "prices: $(ls data/stocks/raw/prices/*.csv 2>/dev/null | wc -l)"
          echo "flows : $(ls data/stocks/raw/krx_flows/*.parquet 2>/dev/null | wc -l)"
          echo "fund  : $(ls data/stocks/raw/fundamentals/*.parquet 2>/dev/null | wc -l)"
          du -sh data/stocks/raw 2>/dev/null || true

      - name: Save cache (canonical raw + curated)
        if: always()
        uses: actions/cache/save@v4
        with:
          path: |
            data/stocks/master
            data/stocks/raw
            data/stocks/curated
            docs/stocks/dart_standard_*.csv
          key: stocks-backfill-${{ env.STOCKS_BACKFILL_CACHE_NS }}-${{ github.ref_name }}-${{ github.run_id }}
