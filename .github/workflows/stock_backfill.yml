name: Stock Backfill RAW (Matrix 2022-2026) + Merge + Curate + Cache (max-parallel=2)

on:
  workflow_dispatch:
    inputs:
      cache_namespace:
        description: "Cache namespace (bump to reset poisoned cache, e.g. v1 -> v2)"
        type: string
        default: "v2"

      min_price_files:
        description: "Sanity: min price CSV files (default 2500)"
        type: string
        default: "2500"
      min_flow_curated_files:
        description: "Sanity: min curated flows_daily.parquet files (default 2000)"
        type: string
        default: "2000"
      min_fund_curated_files:
        description: "Sanity: min curated fundamentals_daily.parquet files (default 2000)"
        type: string
        default: "2000"

      skip_prices:
        description: "Skip price data fetch"
        type: boolean
        default: false
      skip_flows:
        description: "Skip KRX flows fetch"
        type: boolean
        default: false
      skip_fundamentals:
        description: "Skip fundamentals fetch"
        type: boolean
        default: false

      clean_derived:
        description: "Delete derived outputs (curated/analysis/docs) before curations"
        type: boolean
        default: false

permissions:
  contents: read
  actions: write

concurrency:
  group: stock-backfill-raw-matrix-${{ github.ref_name }}
  cancel-in-progress: false

jobs:
  backfill_year:
    runs-on: ubuntu-latest
    timeout-minutes: 330

    strategy:
      fail-fast: false
      max-parallel: 2
      matrix:
        year: [2022, 2023, 2024, 2025, 2026]

    env:
      PYTHONPATH: src
      SAMPLE_MODE: "false"

      STOCKS_BACKFILL_CACHE_NS: ${{ inputs.cache_namespace }}

      # prices
      MAX_WORKERS: "20"
      PRICE_APPEND_IF_EXISTS: "true"
      PRICE_RETRY: "3"
      PRICE_NO_DATA_RETRY: "1"
      PRICE_NO_DATA_SLEEP_BASE: "3.0"
      PRICE_THROTTLE_SEC: "0.06"

      # flows
      MAX_WORKERS_FLOWS: "6"
      KRX_FLOWS_SAVE_FORMAT: "parquet"
      KRX_RETRY: "3"
      KRX_NO_DATA_RETRY: "1"
      KRX_NO_DATA_SLEEP_BASE: "4.0"
      KRX_THROTTLE_SEC: "0.13"
      FLOW_APPEND_IF_EXISTS: "true"

      # fundamentals
      MAX_WORKERS_FUNDAMENTALS: "8"
      FUND_CHUNK_YEARS: "1"
      FUND_SAVE_FORMAT: "parquet"
      FUND_RETRY: "3"
      FUND_EMPTY_RETRY: "2"
      FUND_EMPTY_SLEEP_BASE: "2.5"
      FUND_NO_DATA_RETRY: "1"
      FUND_NO_DATA_SLEEP_BASE: "3.0"
      FUND_THROTTLE_SEC: "0.08"
      FUND_APPEND_IF_EXISTS: "true"

      HEARTBEAT_SEC: "60"

    steps:
      - uses: actions/checkout@v4

      - uses: actions/setup-python@v5
        with:
          python-version: "3.10"

      - name: Install dependencies
        run: |
          pip install --quiet --no-input -U pip
          pip install --quiet --no-input pandas pyarrow pykrx requests tabulate tqdm

      - name: Restore cache (seed)
        id: cache_restore
        uses: actions/cache/restore@v4
        with:
          path: |
            data/stocks/master
            data/stocks/raw
            data/stocks/curated
          key: stocks-backfill-${{ env.STOCKS_BACKFILL_CACHE_NS }}-${{ github.ref_name }}-${{ github.run_id }}
          restore-keys: |
            stocks-backfill-${{ env.STOCKS_BACKFILL_CACHE_NS }}-${{ github.ref_name }}-
            stocks-backfill-${{ env.STOCKS_BACKFILL_CACHE_NS }}-

      - name: "[cleanup] Remove derived outputs (optional)"
        if: ${{ inputs.clean_derived }}
        run: |
          rm -rf data/stocks/curated || true
          rm -rf data/stocks/analysis || true
          rm -rf docs/stocks || true
          mkdir -p data/stocks/curated data/stocks/analysis docs/stocks
          echo "OK: derived outputs cleaned"

      - name: Fetch listings
        continue-on-error: true
        run: |
          python -u src/stocks/fetch_listings.py || true

      - name: Resolve year window for this job
        run: |
          set -euxo pipefail
          Y="${{ matrix.year }}"
          echo "YEAR=$Y" >> $GITHUB_ENV
          echo "START_DATE=${Y}0101" >> $GITHUB_ENV
          echo "END_DATE=${Y}1231" >> $GITHUB_ENV
          echo "This job year=$Y range=${Y}0101~${Y}1231"

      - name: Prepare year-partitioned raw folders
        run: |
          set -euxo pipefail
          mkdir -p data/stocks/raw_year/prices/${YEAR}
          mkdir -p data/stocks/raw_year/krx_flows/${YEAR}
          mkdir -p data/stocks/raw_year/fundamentals/${YEAR}
          mkdir -p docs/stocks

      - name: "Fetch prices (year=${{ matrix.year }})"
        if: ${{ !inputs.skip_prices }}
        continue-on-error: true
        env:
          PROGRESS_JSON: docs/stocks/progress_prices_${{ matrix.year }}.json
          PRICES_RAW_DIR: data/stocks/raw_year/prices/${{ matrix.year }}
        run: |
          set -euxo pipefail
          python -u src/stocks/fetch_prices.py || true
          cat "${PROGRESS_JSON}" || true

      - name: "Fetch KRX flows (year=${{ matrix.year }})"
        if: ${{ !inputs.skip_flows }}
        continue-on-error: true
        env:
          PROGRESS_JSON: docs/stocks/progress_flows_${{ matrix.year }}.json
          FLOWS_RAW_DIR: data/stocks/raw_year/krx_flows/${{ matrix.year }}
        run: |
          set -euxo pipefail
          python -u src/stocks/fetch_krx_flows.py || true
          cat "${PROGRESS_JSON}" || true

      - name: "Fetch fundamentals (year=${{ matrix.year }})"
        if: ${{ !inputs.skip_fundamentals }}
        continue-on-error: true
        env:
          PROGRESS_JSON: docs/stocks/progress_fund_${{ matrix.year }}.json
          FUND_RAW_DIR: data/stocks/raw_year/fundamentals/${{ matrix.year }}
        run: |
          set -euxo pipefail
          python -u src/stocks/fetch_fundamentals.py || true
          cat "${PROGRESS_JSON}" || true

      - name: Upload year raw artifacts
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: raw-year-${{ matrix.year }}
          path: |
            data/stocks/raw_year/prices/${{ matrix.year }}
            data/stocks/raw_year/krx_flows/${{ matrix.year }}
            data/stocks/raw_year/fundamentals/${{ matrix.year }}
            docs/stocks/progress_prices_${{ matrix.year }}.json
            docs/stocks/progress_flows_${{ matrix.year }}.json
            docs/stocks/progress_fund_${{ matrix.year }}.json
          retention-days: 30
          if-no-files-found: warn

  merge_and_cache:
    runs-on: ubuntu-latest
    timeout-minutes: 330
    needs: [backfill_year]

    env:
      PYTHONPATH: src
      STOCKS_BACKFILL_CACHE_NS: ${{ inputs.cache_namespace }}

      MIN_PRICE_FILES: ${{ inputs.min_price_files }}
      MIN_FLOW_CURATED_FILES: ${{ inputs.min_flow_curated_files }}
      MIN_FUND_CURATED_FILES: ${{ inputs.min_fund_curated_files }}

    steps:
      - uses: actions/checkout@v4

      - uses: actions/setup-python@v5
        with:
          python-version: "3.10"

      - name: Install dependencies
        run: |
          pip install --quiet --no-input -U pip
          pip install --quiet --no-input pandas pyarrow tabulate tqdm

      - name: Restore cache (seed canonical raw/curated)
        id: cache_restore
        uses: actions/cache/restore@v4
        with:
          path: |
            data/stocks/master
            data/stocks/raw
            data/stocks/curated
          key: stocks-backfill-${{ env.STOCKS_BACKFILL_CACHE_NS }}-${{ github.ref_name }}-${{ github.run_id }}
          restore-keys: |
            stocks-backfill-${{ env.STOCKS_BACKFILL_CACHE_NS }}-${{ github.ref_name }}-
            stocks-backfill-${{ env.STOCKS_BACKFILL_CACHE_NS }}-

      - name: Download all year artifacts
        uses: actions/download-artifact@v4
        with:
          path: /tmp/raw_year_artifacts

      - name: Merge year-partitioned raw -> canonical raw
        run: |
          set -euxo pipefail
          mkdir -p data/stocks/raw/prices data/stocks/raw/krx_flows data/stocks/raw/fundamentals
          python - <<'PY'
          from pathlib import Path
          import pandas as pd

          ART_ROOT = Path("/tmp/raw_year_artifacts")
          PRICES_CAN = Path("data/stocks/raw/prices")
          FLOWS_CAN  = Path("data/stocks/raw/krx_flows")
          FUND_CAN   = Path("data/stocks/raw/fundamentals")

          YEARS = [2022, 2023, 2024, 2025, 2026]

          def iter_year_dirs(kind: str):
            # actions/download-artifact 구조에 맞춰 광범위 탐색
            for y in YEARS:
              pattern = f"raw-year-{y}/**/data/stocks/raw_year/{kind}/{y}"
              for h in ART_ROOT.glob(pattern):
                if h.is_dir():
                  yield y, h

          def read_csv_safe(p: Path):
            try:
              df = pd.read_csv(p, encoding="utf-8-sig")
              if "date" in df.columns:
                df["date"] = pd.to_datetime(df["date"])
              return df
            except Exception:
              return None

          def read_parquet_safe(p: Path):
            try:
              df = pd.read_parquet(p)
              if "date" in df.columns:
                df["date"] = pd.to_datetime(df["date"])
              return df
            except Exception:
              return None

          def merge_prices():
            by_ticker = {}
            for _, d in iter_year_dirs("prices"):
              for p in d.glob("*.csv"):
                by_ticker.setdefault(p.stem, []).append(p)
            print("[merge] prices tickers:", len(by_ticker))

            for i, (t, files) in enumerate(by_ticker.items(), 1):
              frames = []
              can_path = PRICES_CAN / f"{t}.csv"
              if can_path.exists():
                old = read_csv_safe(can_path)
                if old is not None and not old.empty:
                  frames.append(old)

              for f in sorted(files):
                df = read_csv_safe(f)
                if df is not None and not df.empty:
                  frames.append(df)

              if not frames:
                continue

              out = pd.concat(frames, ignore_index=True)
              if "date" in out.columns:
                out = out.drop_duplicates(subset=["date"], keep="last").sort_values("date")

              can_path.parent.mkdir(parents=True, exist_ok=True)
              out.to_csv(can_path, index=False, encoding="utf-8-sig")

              if i <= 10 or i % 400 == 0:
                print(f"[merge][prices] {i}/{len(by_ticker)} {t} rows={len(out)}")

          def merge_parquet(kind: str, can_dir: Path):
            by_ticker = {}
            for _, d in iter_year_dirs(kind):
              for p in d.glob("*.parquet"):
                by_ticker.setdefault(p.stem, []).append(p)
            print(f"[merge] {kind} tickers:", len(by_ticker))

            for i, (t, files) in enumerate(by_ticker.items(), 1):
              frames = []
              can_path = can_dir / f"{t}.parquet"
              if can_path.exists():
                old = read_parquet_safe(can_path)
                if old is not None and not old.empty:
                  frames.append(old)

              for f in sorted(files):
                df = read_parquet_safe(f)
                if df is not None and not df.empty:
                  frames.append(df)

              if not frames:
                continue

              out = pd.concat(frames, ignore_index=True)
              if "date" in out.columns:
                out = out.drop_duplicates(subset=["date"], keep="last").sort_values("date")

              can_path.parent.mkdir(parents=True, exist_ok=True)
              out.to_parquet(can_path, index=False)

              if i <= 10 or i % 400 == 0:
                print(f"[merge][{kind}] {i}/{len(by_ticker)} {t} rows={len(out)}")

          merge_prices()
          merge_parquet("krx_flows", FLOWS_CAN)
          merge_parquet("fundamentals", FUND_CAN)

          print("[merge] done")
          print("prices files:", len(list(PRICES_CAN.glob("*.csv"))))
          print("flows files :", len(list(FLOWS_CAN.glob("*.parquet"))))
          print("fund files  :", len(list(FUND_CAN.glob("*.parquet"))))
          PY

      # ✅ backfill에서 curated 생성(패널 마트/데일리 sanity 통과를 위해 필수)
      - name: Curate flows & fundamentals (build curated for v2 cache)
        run: |
          python src/stocks/curate_flows.py
          python src/stocks/curate_fundamentals.py
          python - <<'PY'
          from pathlib import Path
          def count_rglob(p: Path, name: str):
            if not p.exists(): return 0
            return sum(1 for _ in p.rglob(name))
          print("[curated] flows_daily =", count_rglob(Path("data/stocks/curated"), "flows_daily.parquet"))
          print("[curated] fundamentals_daily =", count_rglob(Path("data/stocks/curated"), "fundamentals_daily.parquet"))
          PY

      - name: Sanity gate (prices + curated flows/fund)
        id: sanity
        run: |
          python - <<'PY'
          import os
          from pathlib import Path

          def count_glob(p: Path, pat: str):
            if not p.exists(): return 0
            return len(list(p.glob(pat)))

          def count_rglob(p: Path, name: str):
            if not p.exists(): return 0
            return sum(1 for _ in p.rglob(name))

          n_price = count_glob(Path("data/stocks/raw/prices"), "*.csv")
          n_flows_cur = count_rglob(Path("data/stocks/curated"), "flows_daily.parquet")
          n_fund_cur  = count_rglob(Path("data/stocks/curated"), "fundamentals_daily.parquet")

          min_price = int(os.environ.get("MIN_PRICE_FILES", "2500"))
          min_flows = int(os.environ.get("MIN_FLOW_CURATED_FILES", "2000"))
          min_fund  = int(os.environ.get("MIN_FUND_CURATED_FILES", "2000"))

          print(f"[sanity] price_csv_files={n_price} (min={min_price})")
          print(f"[sanity] flows_curated_files={n_flows_cur} (min={min_flows})")
          print(f"[sanity] fund_curated_files={n_fund_cur} (min={min_fund})")

          if n_price < min_price:
            raise SystemExit("[sanity] TOO_FEW_PRICES -> STOP (do NOT save cache)")
          if n_flows_cur < min_flows:
            raise SystemExit("[sanity] TOO_FEW_FLOWS_CURATED -> STOP (do NOT save cache)")
          if n_fund_cur < min_fund:
            raise SystemExit("[sanity] TOO_FEW_FUND_CURATED -> STOP (do NOT save cache)")

          print("[sanity] OK")
          PY

      - name: Save cache (canonical raw + curated) ✅ only when sanity OK
        if: ${{ success() && steps.sanity.outcome == 'success' }}
        uses: actions/cache/save@v4
        with:
          path: |
            data/stocks/master
            data/stocks/raw
            data/stocks/curated
          key: stocks-backfill-${{ env.STOCKS_BACKFILL_CACHE_NS }}-${{ github.ref_name }}-${{ github.run_id }}
