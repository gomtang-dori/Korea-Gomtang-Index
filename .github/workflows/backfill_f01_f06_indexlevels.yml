name: backfill_f01_f06_indexlevels

on:
  workflow_dispatch:
    inputs:
      start:
        description: "BACKFILL_START (YYYYMMDD)"
        required: true
        default: "20180101"
      cache_buffer_days:
        description: "Cache buffer days (int)"
        required: true
        default: "450"
      rolling_days:
        description: "ROLLING_DAYS for scoring (default 1260 = 5y trading days)"
        required: true
        default: "1260"
      min_obs:
        description: "MIN_OBS for scoring (default 252)"
        required: true
        default: "252"
      f01_ma_days:
        description: "F01 moving average days (default 125)"
        required: true
        default: "125"

jobs:
  backfill:
    runs-on: ubuntu-latest
    timeout-minutes: 360

    # ✅ 핵심: src를 모듈 루트로 잡아서 `from lib...` / `from utils...` 임포트가 CI에서도 동작
    env:
      PYTHONPATH: src

    steps:
      - name: Checkout
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Install dependencies
        shell: bash
        run: |
          set -euo pipefail
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      - name: Set backfill range
        shell: bash
        run: |
          set -euo pipefail
          START_INPUT="${{ inputs.start }}"
          END_UTC="$(date -u -d 'yesterday' +%Y%m%d)"

          echo "BACKFILL_START=${START_INPUT}" >> $GITHUB_ENV
          echo "BACKFILL_END=${END_UTC}" >> $GITHUB_ENV
          echo "CACHE_BUFFER_DAYS=${{ inputs.cache_buffer_days }}" >> $GITHUB_ENV

          echo "ROLLING_DAYS=${{ inputs.rolling_days }}" >> $GITHUB_ENV
          echo "MIN_OBS=${{ inputs.min_obs }}" >> $GITHUB_ENV
          echo "F01_MA_DAYS=${{ inputs.f01_ma_days }}" >> $GITHUB_ENV
          echo "WINSOR_P=0.01" >> $GITHUB_ENV

      - name: Cache index levels (FDR) [BACKFILL]
        shell: bash
        run: |
          set -euo pipefail
          python src/caches/cache_index_levels_fdr.py

      - name: Cache VKOSPI (KRX) [BACKFILL]
        shell: bash
        env:
          KRX_AUTH_KEY: ${{ secrets.KRX_AUTH_KEY }}
          KRX_DVRPROD_DD_TRD_URL: ${{ secrets.KRX_DVRPROD_DD_TRD_URL }}
        run: |
          set -euo pipefail
          python src/caches/cache_vkospi_backfill_krx.py

      - name: Calculate F01
        shell: bash
        run: |
          set -euo pipefail
          python src/factors/f01_momentum.py

      - name: Calculate F06
        shell: bash
        run: |
          set -euo pipefail
          python src/factors/f06_vkospi.py

      - name: Quick sanity check (files exist)
        shell: bash
        run: |
          set -euo pipefail
          test -f data/cache/index_levels.parquet
          test -f data/cache/k200_close.parquet
          test -f data/vkospi_level.parquet
          test -f data/factors/f01.parquet
          test -f data/factors/f06.parquet

      - name: Show min/max date (standard)
        shell: bash
        env:
          TARGETS: "data/cache/index_levels.parquet,data/cache/k200_close.parquet,data/vkospi_level.parquet,data/factors/f01.parquet,data/factors/f06.parquet"
          DATE_COL_CANDIDATES: "date,Date,basDt,BASE_DT"
        run: |
          set -euo pipefail
          python - << 'PY'
          import os
          from pathlib import Path
          import pandas as pd

          targets = [t.strip() for t in os.getenv("TARGETS","").split(",") if t.strip()]
          cand_cols = [c.strip() for c in os.getenv("DATE_COL_CANDIDATES","date").split(",") if c.strip()]

          def pick_date_col(df):
            for c in cand_cols:
              if c in df.columns:
                return c
            for c in df.columns:
              if "date" in c.lower():
                return c
            return None

          def read_any(p: Path):
            if p.suffix.lower() == ".parquet":
              return pd.read_parquet(p)
            if p.suffix.lower() == ".csv":
              return pd.read_csv(p)
            raise ValueError(f"Unsupported: {p}")

          print("=== [min/max date] ===")
          for t in targets:
            p = Path(t)
            if not p.exists():
              print(f"[MISS] {t}")
              continue
            try:
              df = read_any(p)
            except Exception as e:
              print(f"[FAIL] {t} {e}")
              continue
            if df is None or len(df) == 0:
              print(f"[EMPTY] {t}")
              continue
            dc = pick_date_col(df)
            if not dc:
              print(f"[NO_DATE_COL] {t} cols={list(df.columns)[:20]}")
              continue
            s = pd.to_datetime(df[dc], errors="coerce").dropna()
            if s.empty:
              print(f"[NO_VALID_DATE] {t}")
              continue
            print(f"[OK] {t} rows={len(df)} date_col={dc} min={s.min()} max={s.max()}")
          PY

      - name: Commit & push (if changed)
        shell: bash
        run: |
          set -euo pipefail
          git config user.name "github-actions"
          git config user.email "github-actions@github.com"

          git add data/cache/index_levels.parquet data/cache/index_levels.csv || true
          git add data/cache/k200_close.parquet data/cache/k200_close.csv || true
          git add data/vkospi_level.parquet data/vkospi_level.csv || true
          git add data/factors/f01.parquet data/factors/f06.parquet || true

          if git diff --staged --quiet; then
            echo "No changes to commit."
            exit 0
          fi

          git commit -m "Backfill F01+F06 (index levels + vkospi) $(date -u +%Y-%m-%d)"

          git stash push -u -m actions-stash-before-rebase || true
          git fetch origin main
          git pull --rebase origin main
          git stash pop || true

          git push origin HEAD:main
